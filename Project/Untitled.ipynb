{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Gridworld(object):\n",
    "    \"\"\"\n",
    "    Gridworld MDP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size, wind, discount):\n",
    "        \"\"\"\n",
    "        grid_size: Grid size. int.\n",
    "        wind: Chance of moving randomly. float.\n",
    "        discount: MDP discount. float.\n",
    "        -> Gridworld\n",
    "        \"\"\"\n",
    "\n",
    "        self.actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = grid_size**2\n",
    "        self.grid_size = grid_size\n",
    "        self.wind = wind\n",
    "        self.discount = discount\n",
    "\n",
    "        # Preconstruct the transition probability array.\n",
    "        self.transition_probability = np.array(\n",
    "            [[[self._transition_probability(i, j, k)\n",
    "               for k in range(self.n_states)]\n",
    "              for j in range(self.n_actions)]\n",
    "             for i in range(self.n_states)])\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Gridworld({}, {}, {})\".format(self.grid_size, self.wind,\n",
    "                                              self.discount)\n",
    "\n",
    "    def feature_vector(self, i, feature_map=\"ident\"):\n",
    "        \"\"\"\n",
    "        Get the feature vector associated with a state integer.\n",
    "        i: State int.\n",
    "        feature_map: Which feature map to use (default ident). String in {ident,\n",
    "            coord, proxi}.\n",
    "        -> Feature vector.\n",
    "        \"\"\"\n",
    "\n",
    "        if feature_map == \"coord\":\n",
    "            f = np.zeros(self.grid_size)\n",
    "            x, y = i % self.grid_size, i // self.grid_size\n",
    "            f[x] += 1\n",
    "            f[y] += 1\n",
    "            return f\n",
    "        if feature_map == \"proxi\":\n",
    "            f = np.zeros(self.n_states)\n",
    "            x, y = i % self.grid_size, i // self.grid_size\n",
    "            for b in range(self.grid_size):\n",
    "                for a in range(self.grid_size):\n",
    "                    dist = abs(x - a) + abs(y - b)\n",
    "                    f[self.point_to_int((a, b))] = dist\n",
    "            return f\n",
    "        # Assume identity map.\n",
    "        f = np.zeros(self.n_states)\n",
    "        f[i] = 1\n",
    "        return f\n",
    "\n",
    "    def feature_matrix(self, feature_map=\"ident\"):\n",
    "        \"\"\"\n",
    "        Get the feature matrix for this gridworld.\n",
    "        feature_map: Which feature map to use (default ident). String in {ident,\n",
    "            coord, proxi}.\n",
    "        -> NumPy array with shape (n_states, d_states).\n",
    "        \"\"\"\n",
    "\n",
    "        features = []\n",
    "        for n in range(self.n_states):\n",
    "            f = self.feature_vector(n, feature_map)\n",
    "            features.append(f)\n",
    "        return np.array(features)\n",
    "\n",
    "    def int_to_point(self, i):\n",
    "        \"\"\"\n",
    "        Convert a state int into the corresponding coordinate.\n",
    "        i: State int.\n",
    "        -> (x, y) int tuple.\n",
    "        \"\"\"\n",
    "\n",
    "        return (i % self.grid_size, i // self.grid_size)\n",
    "\n",
    "    def point_to_int(self, p):\n",
    "        \"\"\"\n",
    "        Convert a coordinate into the corresponding state int.\n",
    "        p: (x, y) tuple.\n",
    "        -> State int.\n",
    "        \"\"\"\n",
    "\n",
    "        return p[0] + p[1]*self.grid_size\n",
    "\n",
    "    def neighbouring(self, i, k):\n",
    "        \"\"\"\n",
    "        Get whether two points neighbour each other. Also returns true if they\n",
    "        are the same point.\n",
    "        i: (x, y) int tuple.\n",
    "        k: (x, y) int tuple.\n",
    "        -> bool.\n",
    "        \"\"\"\n",
    "\n",
    "        return abs(i[0] - k[0]) + abs(i[1] - k[1]) <= 1\n",
    "\n",
    "    def _transition_probability(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Get the probability of transitioning from state i to state k given\n",
    "        action j.\n",
    "        i: State int.\n",
    "        j: Action int.\n",
    "        k: State int.\n",
    "        -> p(s_k | s_i, a_j)\n",
    "        \"\"\"\n",
    "\n",
    "        xi, yi = self.int_to_point(i)\n",
    "        xj, yj = self.actions[j]\n",
    "        xk, yk = self.int_to_point(k)\n",
    "\n",
    "        if not self.neighbouring((xi, yi), (xk, yk)):\n",
    "            return 0.0\n",
    "\n",
    "        # Is k the intended state to move to?\n",
    "        if (xi + xj, yi + yj) == (xk, yk):\n",
    "            return 1 - self.wind + self.wind/self.n_actions\n",
    "\n",
    "        # If these are not the same point, then we can move there by wind.\n",
    "        if (xi, yi) != (xk, yk):\n",
    "            return self.wind/self.n_actions\n",
    "\n",
    "        # If these are the same point, we can only move here by either moving\n",
    "        # off the grid or being blown off the grid. Are we on a corner or not?\n",
    "        if (xi, yi) in {(0, 0), (self.grid_size-1, self.grid_size-1),\n",
    "                        (0, self.grid_size-1), (self.grid_size-1, 0)}:\n",
    "            # Corner.\n",
    "            # Can move off the edge in two directions.\n",
    "            # Did we intend to move off the grid?\n",
    "            if not (0 <= xi + xj < self.grid_size and\n",
    "                    0 <= yi + yj < self.grid_size):\n",
    "                # We intended to move off the grid, so we have the regular\n",
    "                # success chance of staying here plus an extra chance of blowing\n",
    "                # onto the *other* off-grid square.\n",
    "                return 1 - self.wind + 2*self.wind/self.n_actions\n",
    "            else:\n",
    "                # We can blow off the grid in either direction only by wind.\n",
    "                return 2*self.wind/self.n_actions\n",
    "        else:\n",
    "            # Not a corner. Is it an edge?\n",
    "            if (xi not in {0, self.grid_size-1} and\n",
    "                yi not in {0, self.grid_size-1}):\n",
    "                # Not an edge.\n",
    "                return 0.0\n",
    "\n",
    "            # Edge.\n",
    "            # Can only move off the edge in one direction.\n",
    "            # Did we intend to move off the grid?\n",
    "            if not (0 <= xi + xj < self.grid_size and\n",
    "                    0 <= yi + yj < self.grid_size):\n",
    "                # We intended to move off the grid, so we have the regular\n",
    "                # success chance of staying here.\n",
    "                return 1 - self.wind + self.wind/self.n_actions\n",
    "            else:\n",
    "                # We can blow off the grid only by wind.\n",
    "                return self.wind/self.n_actions\n",
    "\n",
    "    def reward(self, state_int):\n",
    "        \"\"\"\n",
    "        Reward for being in state state_int.\n",
    "        state_int: State integer. int.\n",
    "        -> Reward.\n",
    "        \"\"\"\n",
    "\n",
    "        if state_int == self.n_states - 1:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def average_reward(self, n_trajectories, trajectory_length, policy):\n",
    "        \"\"\"\n",
    "        Calculate the average total reward obtained by following a given policy\n",
    "        over n_paths paths.\n",
    "        policy: Map from state integers to action integers.\n",
    "        n_trajectories: Number of trajectories. int.\n",
    "        trajectory_length: Length of an episode. int.\n",
    "        -> Average reward, standard deviation.\n",
    "        \"\"\"\n",
    "\n",
    "        trajectories = self.generate_trajectories(n_trajectories,\n",
    "                                             trajectory_length, policy)\n",
    "        rewards = [[r for _, _, r in trajectory] for trajectory in trajectories]\n",
    "        rewards = np.array(rewards)\n",
    "\n",
    "        # Add up all the rewards to find the total reward.\n",
    "        total_reward = rewards.sum(axis=1)\n",
    "\n",
    "        # Return the average reward and standard deviation.\n",
    "        return total_reward.mean(), total_reward.std()\n",
    "\n",
    "    def optimal_policy(self, state_int):\n",
    "        \"\"\"\n",
    "        The optimal policy for this gridworld.\n",
    "        state_int: What state we are in. int.\n",
    "        -> Action int.\n",
    "        \"\"\"\n",
    "\n",
    "        sx, sy = self.int_to_point(state_int)\n",
    "\n",
    "        if sx < self.grid_size and sy < self.grid_size:\n",
    "            return rn.randint(0, 2)\n",
    "        if sx < self.grid_size-1:\n",
    "            return 0\n",
    "        if sy < self.grid_size-1:\n",
    "            return 1\n",
    "        raise ValueError(\"Unexpected state.\")\n",
    "\n",
    "    def optimal_policy_deterministic(self, state_int):\n",
    "        \"\"\"\n",
    "        Deterministic version of the optimal policy for this gridworld.\n",
    "        state_int: What state we are in. int.\n",
    "        -> Action int.\n",
    "        \"\"\"\n",
    "\n",
    "        sx, sy = self.int_to_point(state_int)\n",
    "        if sx < sy:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def generate_trajectories(self, n_trajectories, trajectory_length, policy,\n",
    "                                    random_start=False):\n",
    "        \"\"\"\n",
    "        Generate n_trajectories trajectories with length trajectory_length,\n",
    "        following the given policy.\n",
    "        n_trajectories: Number of trajectories. int.\n",
    "        trajectory_length: Length of an episode. int.\n",
    "        policy: Map from state integers to action integers.\n",
    "        random_start: Whether to start randomly (default False). bool.\n",
    "        -> [[(state int, action int, reward float)]]\n",
    "        \"\"\"\n",
    "\n",
    "        trajectories = []\n",
    "        for _ in range(n_trajectories):\n",
    "            if random_start:\n",
    "                sx, sy = rn.randint(self.grid_size), rn.randint(self.grid_size)\n",
    "            else:\n",
    "                sx, sy = 0, 0\n",
    "\n",
    "            trajectory = []\n",
    "            for _ in range(trajectory_length):\n",
    "                if rn.random() < self.wind:\n",
    "                    action = self.actions[rn.randint(0, 4)]\n",
    "                else:\n",
    "                    # Follow the given policy.\n",
    "                    action = self.actions[policy(self.point_to_int((sx, sy)))]\n",
    "\n",
    "                if (0 <= sx + action[0] < self.grid_size and\n",
    "                        0 <= sy + action[1] < self.grid_size):\n",
    "                    next_sx = sx + action[0]\n",
    "                    next_sy = sy + action[1]\n",
    "                else:\n",
    "                    next_sx = sx\n",
    "                    next_sy = sy\n",
    "\n",
    "                state_int = self.point_to_int((sx, sy))\n",
    "                action_int = self.actions.index(action)\n",
    "                next_state_int = self.point_to_int((next_sx, next_sy))\n",
    "                reward = self.reward(next_state_int)\n",
    "                trajectory.append((state_int, action_int, reward))\n",
    "\n",
    "                sx = next_sx\n",
    "                sy = next_sy\n",
    "\n",
    "            trajectories.append(trajectory)\n",
    "\n",
    "        return np.array(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def irl(n_states, n_actions, transition_probability, policy, discount, Rmax,\n",
    "        l1):\n",
    "    \"\"\"\n",
    "    Find a reward function with inverse RL as described in Ng & Russell, 2000.\n",
    "    n_states: Number of states. int.\n",
    "    n_actions: Number of actions. int.\n",
    "    transition_probability: NumPy array mapping (state_i, action, state_k) to\n",
    "        the probability of transitioning from state_i to state_k under action.\n",
    "        Shape (N, A, N).\n",
    "    policy: Vector mapping state ints to action ints. Shape (N,).\n",
    "    discount: Discount factor. float.\n",
    "    Rmax: Maximum reward. float.\n",
    "    l1: l1 regularisation. float.\n",
    "    -> Reward vector\n",
    "    \"\"\"\n",
    "\n",
    "    A = set(range(n_actions))  # Set of actions to help manage reordering\n",
    "                               # actions.\n",
    "    # The transition policy convention is different here to the rest of the code\n",
    "    # for legacy reasons; here, we reorder axes to fix this. We expect the\n",
    "    # new probabilities to be of the shape (A, N, N).\n",
    "    transition_probability = np.transpose(transition_probability, (1, 0, 2))\n",
    "\n",
    "    def T(a, s):\n",
    "        \"\"\"\n",
    "        Shorthand for a dot product used a lot in the LP formulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.dot(transition_probability[policy[s], s] -\n",
    "                      transition_probability[a, s],\n",
    "                      np.linalg.inv(np.eye(n_states) -\n",
    "                        discount*transition_probability[policy[s]]))\n",
    "\n",
    "    # This entire function just computes the block matrices used for the LP\n",
    "    # formulation of IRL.\n",
    "\n",
    "    # Minimise c . x.\n",
    "    c = -np.hstack([np.zeros(n_states), np.ones(n_states),\n",
    "                    -l1*np.ones(n_states)])\n",
    "    zero_stack1 = np.zeros((n_states*(n_actions-1), n_states))\n",
    "    T_stack = np.vstack([\n",
    "        -T(a, s)\n",
    "        for s in range(n_states)\n",
    "        for a in A - {policy[s]}\n",
    "    ])\n",
    "    I_stack1 = np.vstack([\n",
    "        np.eye(1, n_states, s)\n",
    "        for s in range(n_states)\n",
    "        for a in A - {policy[s]}\n",
    "    ])\n",
    "    I_stack2 = np.eye(n_states)\n",
    "    zero_stack2 = np.zeros((n_states, n_states))\n",
    "\n",
    "    D_left = np.vstack([T_stack, T_stack, -I_stack2, I_stack2])\n",
    "    D_middle = np.vstack([I_stack1, zero_stack1, zero_stack2, zero_stack2])\n",
    "    D_right = np.vstack([zero_stack1, zero_stack1, -I_stack2, -I_stack2])\n",
    "\n",
    "    D = np.hstack([D_left, D_middle, D_right])\n",
    "    b = np.zeros((n_states*(n_actions-1)*2 + 2*n_states, 1))\n",
    "    bounds = np.array([(None, None)]*2*n_states + [(-Rmax, Rmax)]*n_states)\n",
    "\n",
    "    # We still need to bound R. To do this, we just add\n",
    "    # -I R <= Rmax 1\n",
    "    # I R <= Rmax 1\n",
    "    # So to D we need to add -I and I, and to b we need to add Rmax 1 and Rmax 1\n",
    "    D_bounds = np.hstack([\n",
    "        np.vstack([\n",
    "            -np.eye(n_states),\n",
    "            np.eye(n_states)]),\n",
    "        np.vstack([\n",
    "            np.zeros((n_states, n_states)),\n",
    "            np.zeros((n_states, n_states))]),\n",
    "        np.vstack([\n",
    "            np.zeros((n_states, n_states)),\n",
    "            np.zeros((n_states, n_states))])])\n",
    "    b_bounds = np.vstack([Rmax*np.ones((n_states, 1))]*2)\n",
    "    D = np.vstack((D, D_bounds))\n",
    "    b = np.vstack((b, b_bounds))\n",
    "    A_ub = matrix(D)\n",
    "    b = matrix(b)\n",
    "    c = matrix(c)\n",
    "    results = solvers.lp(c, A_ub, b)\n",
    "    r = np.asarray(results[\"x\"][:n_states], dtype=np.double)\n",
    "\n",
    "    return r.reshape((n_states,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(grid_size, l1):\n",
    "    \"\"\"\n",
    "    Run linear programming inverse reinforcement learning on the gridworld MDP.\n",
    "    Plots the reward function.\n",
    "    grid_size: Grid size. int.\n",
    "    discount: MDP discount factor. float.\n",
    "    \"\"\"\n",
    "\n",
    "    wind = 0.3\n",
    "    trajectory_length = 3*grid_size\n",
    "\n",
    "    gw = Gridworld(grid_size, wind, 0.2)\n",
    "\n",
    "    ground_r = np.array([gw.reward(s) for s in range(gw.n_states)])\n",
    "    policy = [gw.optimal_policy_deterministic(s) for s in range(gw.n_states)]\n",
    "    r = irl(gw.n_states, gw.n_actions, gw.transition_probability,\n",
    "            policy, gw.discount, 1, l1)\n",
    "    \n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pcolor(ground_r.reshape((grid_size, grid_size)))\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Groundtruth reward\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pcolor(r.reshape((grid_size, grid_size)))\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Recovered reward\")\n",
    "    plt.show()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -6.5658e+01  4e+02  2e+00  6e+00  1e+00\n",
      " 1:  4.9645e-01 -1.7914e+01  7e+01  6e-01  2e+00  9e-01\n",
      " 2:  2.3556e-01 -4.1683e+00  1e+01  2e-01  4e-01  3e-01\n",
      " 3: -2.7962e-01 -1.9902e+00  4e+00  6e-02  2e-01  9e-02\n",
      " 4: -4.3327e-01 -1.1672e+00  2e+00  3e-02  7e-02  4e-02\n",
      " 5: -5.1296e-01 -7.6652e-01  6e-01  9e-03  2e-02  9e-03\n",
      " 6: -5.4705e-01 -6.1752e-01  1e-01  2e-03  6e-03  1e-03\n",
      " 7: -5.5911e-01 -5.7495e-01  3e-02  5e-04  1e-03  3e-04\n",
      " 8: -5.5883e-01 -5.6177e-01  6e-03  1e-04  3e-04  3e-05\n",
      " 9: -5.5880e-01 -5.5974e-01  2e-03  3e-05  8e-05  9e-06\n",
      "10: -5.5883e-01 -5.5899e-01  3e-04  5e-06  1e-05  1e-06\n",
      "11: -5.5883e-01 -5.5886e-01  6e-05  9e-07  2e-06  2e-07\n",
      "12: -5.5883e-01 -5.5884e-01  2e-05  3e-07  8e-07  8e-08\n",
      "13: -5.5883e-01 -5.5884e-01  2e-06  2e-08  6e-08  7e-09\n",
      "14: -5.5883e-01 -5.5883e-01  3e-07  5e-09  1e-08  1e-09\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = main(5, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.26549300e-09, -1.59804034e-06, -1.31850600e-10,\n",
       "        -1.51879878e-07, -1.48441546e-07],\n",
       "       [ 6.95960136e-04,  4.11082582e-03, -3.52689099e-09,\n",
       "        -7.94960710e-10, -9.18166253e-10],\n",
       "       [-1.02548836e-05,  4.35343747e-03,  2.56610326e-02,\n",
       "        -1.95641595e-09, -6.61843823e-10],\n",
       "       [-1.09650591e-05, -2.65271372e-09,  2.70904208e-02,\n",
       "         1.60190629e-01, -1.10974051e-09],\n",
       "       [-1.25445045e-09,  3.15285900e-09,  1.03220907e-09,\n",
       "         1.42886026e-01,  1.00000000e+00]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.reshape(5,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
