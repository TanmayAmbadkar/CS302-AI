{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Gridworld(object):\n",
    "    \"\"\"\n",
    "    Gridworld MDP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size, wind, discount):\n",
    "        \"\"\"\n",
    "        grid_size: Grid size. int.\n",
    "        wind: Chance of moving randomly. float.\n",
    "        discount: MDP discount. float.\n",
    "        -> Gridworld\n",
    "        \"\"\"\n",
    "\n",
    "        self.actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = grid_size**2\n",
    "        self.grid_size = grid_size\n",
    "        self.wind = wind\n",
    "        self.discount = discount\n",
    "\n",
    "        # Preconstruct the transition probability array.\n",
    "        self.transition_probability = np.array(\n",
    "            [[[self._transition_probability(i, j, k)\n",
    "               for k in range(self.n_states)]\n",
    "              for j in range(self.n_actions)]\n",
    "             for i in range(self.n_states)])\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Gridworld({}, {}, {})\".format(self.grid_size, self.wind,\n",
    "                                              self.discount)\n",
    "\n",
    "    def feature_vector(self, i, feature_map=\"ident\"):\n",
    "        \"\"\"\n",
    "        Get the feature vector associated with a state integer.\n",
    "        i: State int.\n",
    "        feature_map: Which feature map to use (default ident). String in {ident,\n",
    "            coord, proxi}.\n",
    "        -> Feature vector.\n",
    "        \"\"\"\n",
    "\n",
    "        if feature_map == \"coord\":\n",
    "            f = np.zeros(self.grid_size)\n",
    "            x, y = i % self.grid_size, i // self.grid_size\n",
    "            f[x] += 1\n",
    "            f[y] += 1\n",
    "            return f\n",
    "        if feature_map == \"proxi\":\n",
    "            f = np.zeros(self.n_states)\n",
    "            x, y = i % self.grid_size, i // self.grid_size\n",
    "            for b in range(self.grid_size):\n",
    "                for a in range(self.grid_size):\n",
    "                    dist = abs(x - a) + abs(y - b)\n",
    "                    f[self.point_to_int((a, b))] = dist\n",
    "            return f\n",
    "        # Assume identity map.\n",
    "        f = np.zeros(self.n_states)\n",
    "        f[i] = 1\n",
    "        return f\n",
    "\n",
    "    def feature_matrix(self, feature_map=\"ident\"):\n",
    "        \"\"\"\n",
    "        Get the feature matrix for this gridworld.\n",
    "        feature_map: Which feature map to use (default ident). String in {ident,\n",
    "            coord, proxi}.\n",
    "        -> NumPy array with shape (n_states, d_states).\n",
    "        \"\"\"\n",
    "\n",
    "        features = []\n",
    "        for n in range(self.n_states):\n",
    "            f = self.feature_vector(n, feature_map)\n",
    "            features.append(f)\n",
    "        return np.array(features)\n",
    "\n",
    "    def int_to_point(self, i):\n",
    "        \"\"\"\n",
    "        Convert a state int into the corresponding coordinate.\n",
    "        i: State int.\n",
    "        -> (x, y) int tuple.\n",
    "        \"\"\"\n",
    "\n",
    "        return (i % self.grid_size, i // self.grid_size)\n",
    "\n",
    "    def point_to_int(self, p):\n",
    "        \"\"\"\n",
    "        Convert a coordinate into the corresponding state int.\n",
    "        p: (x, y) tuple.\n",
    "        -> State int.\n",
    "        \"\"\"\n",
    "\n",
    "        return p[0] + p[1]*self.grid_size\n",
    "\n",
    "    def neighbouring(self, i, k):\n",
    "        \"\"\"\n",
    "        Get whether two points neighbour each other. Also returns true if they\n",
    "        are the same point.\n",
    "        i: (x, y) int tuple.\n",
    "        k: (x, y) int tuple.\n",
    "        -> bool.\n",
    "        \"\"\"\n",
    "\n",
    "        return abs(i[0] - k[0]) + abs(i[1] - k[1]) <= 1\n",
    "\n",
    "    def _transition_probability(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Get the probability of transitioning from state i to state k given\n",
    "        action j.\n",
    "        i: State int.\n",
    "        j: Action int.\n",
    "        k: State int.\n",
    "        -> p(s_k | s_i, a_j)\n",
    "        \"\"\"\n",
    "\n",
    "        xi, yi = self.int_to_point(i)\n",
    "        xj, yj = self.actions[j]\n",
    "        xk, yk = self.int_to_point(k)\n",
    "\n",
    "        if not self.neighbouring((xi, yi), (xk, yk)):\n",
    "            return 0.0\n",
    "\n",
    "        # Is k the intended state to move to?\n",
    "        if (xi + xj, yi + yj) == (xk, yk):\n",
    "            return 1 - self.wind + self.wind/self.n_actions\n",
    "\n",
    "        # If these are not the same point, then we can move there by wind.\n",
    "        if (xi, yi) != (xk, yk):\n",
    "            return self.wind/self.n_actions\n",
    "\n",
    "        # If these are the same point, we can only move here by either moving\n",
    "        # off the grid or being blown off the grid. Are we on a corner or not?\n",
    "        if (xi, yi) in {(0, 0), (self.grid_size-1, self.grid_size-1),\n",
    "                        (0, self.grid_size-1), (self.grid_size-1, 0)}:\n",
    "            # Corner.\n",
    "            # Can move off the edge in two directions.\n",
    "            # Did we intend to move off the grid?\n",
    "            if not (0 <= xi + xj < self.grid_size and\n",
    "                    0 <= yi + yj < self.grid_size):\n",
    "                # We intended to move off the grid, so we have the regular\n",
    "                # success chance of staying here plus an extra chance of blowing\n",
    "                # onto the *other* off-grid square.\n",
    "                return 1 - self.wind + 2*self.wind/self.n_actions\n",
    "            else:\n",
    "                # We can blow off the grid in either direction only by wind.\n",
    "                return 2*self.wind/self.n_actions\n",
    "        else:\n",
    "            # Not a corner. Is it an edge?\n",
    "            if (xi not in {0, self.grid_size-1} and\n",
    "                yi not in {0, self.grid_size-1}):\n",
    "                # Not an edge.\n",
    "                return 0.0\n",
    "\n",
    "            # Edge.\n",
    "            # Can only move off the edge in one direction.\n",
    "            # Did we intend to move off the grid?\n",
    "            if not (0 <= xi + xj < self.grid_size and\n",
    "                    0 <= yi + yj < self.grid_size):\n",
    "                # We intended to move off the grid, so we have the regular\n",
    "                # success chance of staying here.\n",
    "                return 1 - self.wind + self.wind/self.n_actions\n",
    "            else:\n",
    "                # We can blow off the grid only by wind.\n",
    "                return self.wind/self.n_actions\n",
    "\n",
    "    def reward(self, state_int):\n",
    "        \"\"\"\n",
    "        Reward for being in state state_int.\n",
    "        state_int: State integer. int.\n",
    "        -> Reward.\n",
    "        \"\"\"\n",
    "\n",
    "        if state_int == self.n_states - 1:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def average_reward(self, n_trajectories, trajectory_length, policy):\n",
    "        \"\"\"\n",
    "        Calculate the average total reward obtained by following a given policy\n",
    "        over n_paths paths.\n",
    "        policy: Map from state integers to action integers.\n",
    "        n_trajectories: Number of trajectories. int.\n",
    "        trajectory_length: Length of an episode. int.\n",
    "        -> Average reward, standard deviation.\n",
    "        \"\"\"\n",
    "\n",
    "        trajectories = self.generate_trajectories(n_trajectories,\n",
    "                                             trajectory_length, policy)\n",
    "        rewards = [[r for _, _, r in trajectory] for trajectory in trajectories]\n",
    "        rewards = np.array(rewards)\n",
    "\n",
    "        # Add up all the rewards to find the total reward.\n",
    "        total_reward = rewards.sum(axis=1)\n",
    "\n",
    "        # Return the average reward and standard deviation.\n",
    "        return total_reward.mean(), total_reward.std()\n",
    "\n",
    "    def optimal_policy(self, state_int):\n",
    "        \"\"\"\n",
    "        The optimal policy for this gridworld.\n",
    "        state_int: What state we are in. int.\n",
    "        -> Action int.\n",
    "        \"\"\"\n",
    "\n",
    "        sx, sy = self.int_to_point(state_int)\n",
    "\n",
    "        if sx < self.grid_size and sy < self.grid_size:\n",
    "            return rn.randint(0, 2)\n",
    "        if sx < self.grid_size-1:\n",
    "            return 0\n",
    "        if sy < self.grid_size-1:\n",
    "            return 1\n",
    "        raise ValueError(\"Unexpected state.\")\n",
    "\n",
    "    def optimal_policy_deterministic(self, state_int):\n",
    "        \"\"\"\n",
    "        Deterministic version of the optimal policy for this gridworld.\n",
    "        state_int: What state we are in. int.\n",
    "        -> Action int.\n",
    "        \"\"\"\n",
    "\n",
    "        sx, sy = self.int_to_point(state_int)\n",
    "        if sx < sy:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def generate_trajectories(self, n_trajectories, trajectory_length, policy,\n",
    "                                    random_start=False):\n",
    "        \"\"\"\n",
    "        Generate n_trajectories trajectories with length trajectory_length,\n",
    "        following the given policy.\n",
    "        n_trajectories: Number of trajectories. int.\n",
    "        trajectory_length: Length of an episode. int.\n",
    "        policy: Map from state integers to action integers.\n",
    "        random_start: Whether to start randomly (default False). bool.\n",
    "        -> [[(state int, action int, reward float)]]\n",
    "        \"\"\"\n",
    "\n",
    "        trajectories = []\n",
    "        for _ in range(n_trajectories):\n",
    "            if random_start:\n",
    "                sx, sy = rn.randint(self.grid_size), rn.randint(self.grid_size)\n",
    "            else:\n",
    "                sx, sy = 0, 0\n",
    "\n",
    "            trajectory = []\n",
    "            for _ in range(trajectory_length):\n",
    "                if rn.random() < self.wind:\n",
    "                    action = self.actions[rn.randint(0, 4)]\n",
    "                else:\n",
    "                    # Follow the given policy.\n",
    "                    action = self.actions[policy(self.point_to_int((sx, sy)))]\n",
    "\n",
    "                if (0 <= sx + action[0] < self.grid_size and\n",
    "                        0 <= sy + action[1] < self.grid_size):\n",
    "                    next_sx = sx + action[0]\n",
    "                    next_sy = sy + action[1]\n",
    "                else:\n",
    "                    next_sx = sx\n",
    "                    next_sy = sy\n",
    "\n",
    "                state_int = self.point_to_int((sx, sy))\n",
    "                action_int = self.actions.index(action)\n",
    "                next_state_int = self.point_to_int((next_sx, next_sy))\n",
    "                reward = self.reward(next_state_int)\n",
    "                trajectory.append((state_int, action_int, reward))\n",
    "\n",
    "                sx = next_sx\n",
    "                sy = next_sy\n",
    "\n",
    "            trajectories.append(trajectory)\n",
    "\n",
    "        return np.array(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def irl(n_states, n_actions, transition_probability, policy, discount, Rmax,\n",
    "        l1):\n",
    "    \"\"\"\n",
    "    Find a reward function with inverse RL as described in Ng & Russell, 2000.\n",
    "    n_states: Number of states. int.\n",
    "    n_actions: Number of actions. int.\n",
    "    transition_probability: NumPy array mapping (state_i, action, state_k) to\n",
    "        the probability of transitioning from state_i to state_k under action.\n",
    "        Shape (N, A, N).\n",
    "    policy: Vector mapping state ints to action ints. Shape (N,).\n",
    "    discount: Discount factor. float.\n",
    "    Rmax: Maximum reward. float.\n",
    "    l1: l1 regularisation. float.\n",
    "    -> Reward vector\n",
    "    \"\"\"\n",
    "\n",
    "    A = set(range(n_actions))  # Set of actions to help manage reordering\n",
    "                               # actions.\n",
    "    # The transition policy convention is different here to the rest of the code\n",
    "    # for legacy reasons; here, we reorder axes to fix this. We expect the\n",
    "    # new probabilities to be of the shape (A, N, N).\n",
    "    transition_probability = np.transpose(transition_probability, (1, 0, 2))\n",
    "\n",
    "    def T(a, s):\n",
    "        \"\"\"\n",
    "        Shorthand for a dot product used a lot in the LP formulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.dot(transition_probability[policy[s], s] -\n",
    "                      transition_probability[a, s],\n",
    "                      np.linalg.inv(np.eye(n_states) -\n",
    "                        discount*transition_probability[policy[s]]))\n",
    "\n",
    "    # This entire function just computes the block matrices used for the LP\n",
    "    # formulation of IRL.\n",
    "\n",
    "    # Minimise c . x.\n",
    "    c = -np.hstack([np.zeros(n_states), np.ones(n_states),\n",
    "                    -l1*np.ones(n_states)])\n",
    "    zero_stack1 = np.zeros((n_states*(n_actions-1), n_states))\n",
    "    T_stack = np.vstack([\n",
    "        -T(a, s)\n",
    "        for s in range(n_states)\n",
    "        for a in A - {policy[s]}\n",
    "    ])\n",
    "    I_stack1 = np.vstack([\n",
    "        np.eye(1, n_states, s)\n",
    "        for s in range(n_states)\n",
    "        for a in A - {policy[s]}\n",
    "    ])\n",
    "    I_stack2 = np.eye(n_states)\n",
    "    zero_stack2 = np.zeros((n_states, n_states))\n",
    "\n",
    "    D_left = np.vstack([T_stack, T_stack, -I_stack2, I_stack2])\n",
    "    D_middle = np.vstack([I_stack1, zero_stack1, zero_stack2, zero_stack2])\n",
    "    D_right = np.vstack([zero_stack1, zero_stack1, -I_stack2, -I_stack2])\n",
    "\n",
    "    D = np.hstack([D_left, D_middle, D_right])\n",
    "    b = np.zeros((n_states*(n_actions-1)*2 + 2*n_states, 1))\n",
    "    bounds = np.array([(None, None)]*2*n_states + [(-Rmax, Rmax)]*n_states)\n",
    "\n",
    "    # We still need to bound R. To do this, we just add\n",
    "    # -I R <= Rmax 1\n",
    "    # I R <= Rmax 1\n",
    "    # So to D we need to add -I and I, and to b we need to add Rmax 1 and Rmax 1\n",
    "    D_bounds = np.hstack([\n",
    "        np.vstack([\n",
    "            -np.eye(n_states),\n",
    "            np.eye(n_states)]),\n",
    "        np.vstack([\n",
    "            np.zeros((n_states, n_states)),\n",
    "            np.zeros((n_states, n_states))]),\n",
    "        np.vstack([\n",
    "            np.zeros((n_states, n_states)),\n",
    "            np.zeros((n_states, n_states))])])\n",
    "    b_bounds = np.vstack([Rmax*np.ones((n_states, 1))]*2)\n",
    "    D = np.vstack((D, D_bounds))\n",
    "    b = np.vstack((b, b_bounds))\n",
    "    A_ub = matrix(D)\n",
    "    b = matrix(b)\n",
    "    c = matrix(c)\n",
    "    results = solvers.lp(c, A_ub, b)\n",
    "    r = np.asarray(results[\"x\"][:n_states], dtype=np.double)\n",
    "\n",
    "    return r.reshape((n_states,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(grid_size, l1):\n",
    "    \"\"\"\n",
    "    Run linear programming inverse reinforcement learning on the gridworld MDP.\n",
    "    Plots the reward function.\n",
    "    grid_size: Grid size. int.\n",
    "    discount: MDP discount factor. float.\n",
    "    \"\"\"\n",
    "\n",
    "    wind = 0.3\n",
    "    trajectory_length = 3*grid_size\n",
    "\n",
    "    gw = Gridworld(grid_size, wind, 0.2)\n",
    "\n",
    "    ground_r = np.array([gw.reward(s) for s in range(gw.n_states)])\n",
    "    policy = [gw.optimal_policy_deterministic(s) for s in range(gw.n_states)]\n",
    "    r = irl(gw.n_states, gw.n_actions, gw.transition_probability,\n",
    "            policy, gw.discount, 1, l1)\n",
    "    \n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pcolor(ground_r.reshape((grid_size, grid_size)))\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Groundtruth reward\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pcolor(r.reshape((grid_size, grid_size)))\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Recovered reward\")\n",
    "    plt.show()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -6.5658e+01  4e+02  2e+00  5e+00  1e+00\n",
      " 1:  9.0128e-01 -1.8795e+01  8e+01  7e-01  1e+00  9e-01\n",
      " 2:  6.0531e-01 -2.9999e+00  1e+01  1e-01  3e-01  2e-01\n",
      " 3:  2.5924e-01 -1.8320e+00  5e+00  7e-02  2e-01  1e-01\n",
      " 4:  6.9146e-03 -7.9594e-01  2e+00  3e-02  6e-02  5e-02\n",
      " 5: -7.8696e-02 -2.7452e-01  5e-01  7e-03  1e-02  8e-03\n",
      " 6: -7.8530e-02 -1.2390e-01  1e-01  2e-03  3e-03  5e-04\n",
      " 7: -8.2120e-02 -9.0456e-02  2e-02  3e-04  6e-04  9e-05\n",
      " 8: -8.1978e-02 -8.5260e-02  7e-03  1e-04  2e-04  3e-05\n",
      " 9: -8.1872e-02 -8.2600e-02  2e-03  2e-05  5e-05  7e-06\n",
      "10: -8.1815e-02 -8.1949e-02  3e-04  5e-06  1e-05  1e-06\n",
      "11: -8.1798e-02 -8.1833e-02  8e-05  1e-06  3e-06  3e-07\n",
      "12: -8.1793e-02 -8.1799e-02  1e-05  2e-07  4e-07  5e-08\n",
      "13: -8.1792e-02 -8.1793e-02  2e-06  4e-08  8e-08  1e-08\n",
      "14: -8.1792e-02 -8.1792e-02  3e-07  5e-09  1e-08  1e-09\n",
      "15: -8.1792e-02 -8.1792e-02  4e-08  6e-10  1e-09  2e-10\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = main(5, 1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.32596272e-10, -4.48636228e-10, -3.92633035e-10,\n",
       "        -4.11464827e-10, -4.16337716e-10],\n",
       "       [-4.11445417e-11,  1.43393253e-10, -3.04976086e-11,\n",
       "        -1.44699119e-11, -2.66801631e-11],\n",
       "       [-2.90299060e-10,  2.63973521e-10,  4.76106504e-10,\n",
       "         3.47919138e-11,  5.18162232e-11],\n",
       "       [-3.06566120e-10, -1.43820210e-12,  6.83834900e-10,\n",
       "         1.28596494e-09,  6.74114445e-11],\n",
       "       [-3.48716678e-10,  5.93309377e-12,  6.14728881e-11,\n",
       "         2.06827278e-09,  7.79395792e-09]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.reshape(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
