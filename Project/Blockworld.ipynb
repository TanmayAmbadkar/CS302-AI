{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Gridworld(object):\n",
    "    \"\"\"\n",
    "    Gridworld MDP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size, wind, discount):\n",
    "        \"\"\"\n",
    "        grid_size: Grid size. int.\n",
    "        wind: Chance of moving randomly. float.\n",
    "        discount: MDP discount. float.\n",
    "        -> Gridworld\n",
    "        \"\"\"\n",
    "\n",
    "        self.actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = grid_size**2\n",
    "        self.grid_size = grid_size\n",
    "        self.wind = wind\n",
    "        self.discount = discount\n",
    "\n",
    "        # Preconstruct the transition probability array.\n",
    "        self.transition_probability = np.array(\n",
    "            [[[self._transition_probability(i, j, k)\n",
    "               for k in range(self.n_states)]\n",
    "              for j in range(self.n_actions)]\n",
    "             for i in range(self.n_states)])\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Gridworld({}, {}, {})\".format(self.grid_size, self.wind,\n",
    "                                              self.discount)\n",
    "\n",
    "    def feature_vector(self, i, feature_map=\"ident\"):\n",
    "        \"\"\"\n",
    "        Get the feature vector associated with a state integer.\n",
    "        i: State int.\n",
    "        feature_map: Which feature map to use (default ident). String in {ident,\n",
    "            coord, proxi}.\n",
    "        -> Feature vector.\n",
    "        \"\"\"\n",
    "\n",
    "        if feature_map == \"coord\":\n",
    "            f = np.zeros(self.grid_size)\n",
    "            x, y = i % self.grid_size, i // self.grid_size\n",
    "            f[x] += 1\n",
    "            f[y] += 1\n",
    "            return f\n",
    "        if feature_map == \"proxi\":\n",
    "            f = np.zeros(self.n_states)\n",
    "            x, y = i % self.grid_size, i // self.grid_size\n",
    "            for b in range(self.grid_size):\n",
    "                for a in range(self.grid_size):\n",
    "                    dist = abs(x - a) + abs(y - b)\n",
    "                    f[self.point_to_int((a, b))] = dist\n",
    "            return f\n",
    "        # Assume identity map.\n",
    "        f = np.zeros(self.n_states)\n",
    "        f[i] = 1\n",
    "        return f\n",
    "\n",
    "    def feature_matrix(self, feature_map=\"ident\"):\n",
    "        \"\"\"\n",
    "        Get the feature matrix for this gridworld.\n",
    "        feature_map: Which feature map to use (default ident). String in {ident,\n",
    "            coord, proxi}.\n",
    "        -> NumPy array with shape (n_states, d_states).\n",
    "        \"\"\"\n",
    "\n",
    "        features = []\n",
    "        for n in range(self.n_states):\n",
    "            f = self.feature_vector(n, feature_map)\n",
    "            features.append(f)\n",
    "        return np.array(features)\n",
    "\n",
    "    def int_to_point(self, i):\n",
    "        \"\"\"\n",
    "        Convert a state int into the corresponding coordinate.\n",
    "        i: State int.\n",
    "        -> (x, y) int tuple.\n",
    "        \"\"\"\n",
    "\n",
    "        return (i % self.grid_size, i // self.grid_size)\n",
    "\n",
    "    def point_to_int(self, p):\n",
    "        \"\"\"\n",
    "        Convert a coordinate into the corresponding state int.\n",
    "        p: (x, y) tuple.\n",
    "        -> State int.\n",
    "        \"\"\"\n",
    "\n",
    "        return p[0] + p[1]*self.grid_size\n",
    "\n",
    "    def neighbouring(self, i, k):\n",
    "        \"\"\"\n",
    "        Get whether two points neighbour each other. Also returns true if they\n",
    "        are the same point.\n",
    "        i: (x, y) int tuple.\n",
    "        k: (x, y) int tuple.\n",
    "        -> bool.\n",
    "        \"\"\"\n",
    "\n",
    "        return abs(i[0] - k[0]) + abs(i[1] - k[1]) <= 1\n",
    "\n",
    "    def _transition_probability(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Get the probability of transitioning from state i to state k given\n",
    "        action j.\n",
    "        i: State int.\n",
    "        j: Action int.\n",
    "        k: State int.\n",
    "        -> p(s_k | s_i, a_j)\n",
    "        \"\"\"\n",
    "\n",
    "        xi, yi = self.int_to_point(i)\n",
    "        xj, yj = self.actions[j]\n",
    "        xk, yk = self.int_to_point(k)\n",
    "\n",
    "        if not self.neighbouring((xi, yi), (xk, yk)):\n",
    "            return 0.0\n",
    "\n",
    "        # Is k the intended state to move to?\n",
    "        if (xi + xj, yi + yj) == (xk, yk):\n",
    "            return 1 - self.wind + self.wind/self.n_actions\n",
    "\n",
    "        # If these are not the same point, then we can move there by wind.\n",
    "        if (xi, yi) != (xk, yk):\n",
    "            return self.wind/self.n_actions\n",
    "\n",
    "        # If these are the same point, we can only move here by either moving\n",
    "        # off the grid or being blown off the grid. Are we on a corner or not?\n",
    "        if (xi, yi) in {(0, 0), (self.grid_size-1, self.grid_size-1),\n",
    "                        (0, self.grid_size-1), (self.grid_size-1, 0)}:\n",
    "            # Corner.\n",
    "            # Can move off the edge in two directions.\n",
    "            # Did we intend to move off the grid?\n",
    "            if not (0 <= xi + xj < self.grid_size and\n",
    "                    0 <= yi + yj < self.grid_size):\n",
    "                # We intended to move off the grid, so we have the regular\n",
    "                # success chance of staying here plus an extra chance of blowing\n",
    "                # onto the *other* off-grid square.\n",
    "                return 1 - self.wind + 2*self.wind/self.n_actions\n",
    "            else:\n",
    "                # We can blow off the grid in either direction only by wind.\n",
    "                return 2*self.wind/self.n_actions\n",
    "        else:\n",
    "            # Not a corner. Is it an edge?\n",
    "            if (xi not in {0, self.grid_size-1} and\n",
    "                yi not in {0, self.grid_size-1}):\n",
    "                # Not an edge.\n",
    "                return 0.0\n",
    "\n",
    "            # Edge.\n",
    "            # Can only move off the edge in one direction.\n",
    "            # Did we intend to move off the grid?\n",
    "            if not (0 <= xi + xj < self.grid_size and\n",
    "                    0 <= yi + yj < self.grid_size):\n",
    "                # We intended to move off the grid, so we have the regular\n",
    "                # success chance of staying here.\n",
    "                return 1 - self.wind + self.wind/self.n_actions\n",
    "            else:\n",
    "                # We can blow off the grid only by wind.\n",
    "                return self.wind/self.n_actions\n",
    "\n",
    "    def reward(self, state_int):\n",
    "        \"\"\"\n",
    "        Reward for being in state state_int.\n",
    "        state_int: State integer. int.\n",
    "        -> Reward.\n",
    "        \"\"\"\n",
    "\n",
    "        if state_int == self.n_states - 1:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def average_reward(self, n_trajectories, trajectory_length, policy):\n",
    "        \"\"\"\n",
    "        Calculate the average total reward obtained by following a given policy\n",
    "        over n_paths paths.\n",
    "        policy: Map from state integers to action integers.\n",
    "        n_trajectories: Number of trajectories. int.\n",
    "        trajectory_length: Length of an episode. int.\n",
    "        -> Average reward, standard deviation.\n",
    "        \"\"\"\n",
    "\n",
    "        trajectories = self.generate_trajectories(n_trajectories,\n",
    "                                             trajectory_length, policy)\n",
    "        rewards = [[r for _, _, r in trajectory] for trajectory in trajectories]\n",
    "        rewards = np.array(rewards)\n",
    "\n",
    "        # Add up all the rewards to find the total reward.\n",
    "        total_reward = rewards.sum(axis=1)\n",
    "\n",
    "        # Return the average reward and standard deviation.\n",
    "        return total_reward.mean(), total_reward.std()\n",
    "\n",
    "    def optimal_policy(self, state_int):\n",
    "        \"\"\"\n",
    "        The optimal policy for this gridworld.\n",
    "        state_int: What state we are in. int.\n",
    "        -> Action int.\n",
    "        \"\"\"\n",
    "\n",
    "        sx, sy = self.int_to_point(state_int)\n",
    "\n",
    "        if sx < self.grid_size and sy < self.grid_size:\n",
    "            return rn.randint(0, 2)\n",
    "        if sx < self.grid_size-1:\n",
    "            return 0\n",
    "        if sy < self.grid_size-1:\n",
    "            return 1\n",
    "        raise ValueError(\"Unexpected state.\")\n",
    "\n",
    "    def optimal_policy_deterministic(self, state_int):\n",
    "        \"\"\"\n",
    "        Deterministic version of the optimal policy for this gridworld.\n",
    "        state_int: What state we are in. int.\n",
    "        -> Action int.\n",
    "        \"\"\"\n",
    "\n",
    "        sx, sy = self.int_to_point(state_int)\n",
    "        if sx < sy:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def generate_trajectories(self, n_trajectories, trajectory_length, policy,\n",
    "                                    random_start=False):\n",
    "        \"\"\"\n",
    "        Generate n_trajectories trajectories with length trajectory_length,\n",
    "        following the given policy.\n",
    "        n_trajectories: Number of trajectories. int.\n",
    "        trajectory_length: Length of an episode. int.\n",
    "        policy: Map from state integers to action integers.\n",
    "        random_start: Whether to start randomly (default False). bool.\n",
    "        -> [[(state int, action int, reward float)]]\n",
    "        \"\"\"\n",
    "\n",
    "        trajectories = []\n",
    "        for _ in range(n_trajectories):\n",
    "            if random_start:\n",
    "                sx, sy = rn.randint(self.grid_size), rn.randint(self.grid_size)\n",
    "            else:\n",
    "                sx, sy = 0, 0\n",
    "\n",
    "            trajectory = []\n",
    "            for _ in range(trajectory_length):\n",
    "                if rn.random() < self.wind:\n",
    "                    action = self.actions[rn.randint(0, 4)]\n",
    "                else:\n",
    "                    # Follow the given policy.\n",
    "                    action = self.actions[policy(self.point_to_int((sx, sy)))]\n",
    "\n",
    "                if (0 <= sx + action[0] < self.grid_size and\n",
    "                        0 <= sy + action[1] < self.grid_size):\n",
    "                    next_sx = sx + action[0]\n",
    "                    next_sy = sy + action[1]\n",
    "                else:\n",
    "                    next_sx = sx\n",
    "                    next_sy = sy\n",
    "\n",
    "                state_int = self.point_to_int((sx, sy))\n",
    "                action_int = self.actions.index(action)\n",
    "                next_state_int = self.point_to_int((next_sx, next_sy))\n",
    "                reward = self.reward(next_state_int)\n",
    "                trajectory.append((state_int, action_int, reward))\n",
    "\n",
    "                sx = next_sx\n",
    "                sy = next_sy\n",
    "\n",
    "            trajectories.append(trajectory)\n",
    "\n",
    "        return np.array(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def irl(n_states, n_actions, transition_probability, policy, discount, Rmax,\n",
    "        l1):\n",
    "    \"\"\"\n",
    "    Find a reward function with inverse RL as described in Ng & Russell, 2000.\n",
    "    n_states: Number of states. int.\n",
    "    n_actions: Number of actions. int.\n",
    "    transition_probability: NumPy array mapping (state_i, action, state_k) to\n",
    "        the probability of transitioning from state_i to state_k under action.\n",
    "        Shape (N, A, N).\n",
    "    policy: Vector mapping state ints to action ints. Shape (N,).\n",
    "    discount: Discount factor. float.\n",
    "    Rmax: Maximum reward. float.\n",
    "    l1: l1 regularisation. float.\n",
    "    -> Reward vector\n",
    "    \"\"\"\n",
    "\n",
    "    A = set(range(n_actions))  # Set of actions to help manage reordering\n",
    "                               # actions.\n",
    "    # The transition policy convention is different here to the rest of the code\n",
    "    # for legacy reasons; here, we reorder axes to fix this. We expect the\n",
    "    # new probabilities to be of the shape (A, N, N).\n",
    "    transition_probability = np.transpose(transition_probability, (1, 0, 2))\n",
    "\n",
    "    def T(a, s):\n",
    "        \"\"\"\n",
    "        Shorthand for a dot product used a lot in the LP formulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.dot(transition_probability[policy[s], s] -\n",
    "                      transition_probability[a, s],\n",
    "                      np.linalg.inv(np.eye(n_states) -\n",
    "                        discount*transition_probability[policy[s]]))\n",
    "\n",
    "    # This entire function just computes the block matrices used for the LP\n",
    "    # formulation of IRL.\n",
    "\n",
    "    # Minimise c . x.\n",
    "    c = -np.hstack([np.zeros(n_states), np.ones(n_states),\n",
    "                    -l1*np.ones(n_states)])\n",
    "    zero_stack1 = np.zeros((n_states*(n_actions-1), n_states))\n",
    "    T_stack = np.vstack([\n",
    "        -T(a, s)\n",
    "        for s in range(n_states)\n",
    "        for a in A - {policy[s]}\n",
    "    ])\n",
    "    I_stack1 = np.vstack([\n",
    "        np.eye(1, n_states, s)\n",
    "        for s in range(n_states)\n",
    "        for a in A - {policy[s]}\n",
    "    ])\n",
    "    I_stack2 = np.eye(n_states)\n",
    "    zero_stack2 = np.zeros((n_states, n_states))\n",
    "\n",
    "    D_left = np.vstack([T_stack, T_stack, -I_stack2, I_stack2])\n",
    "    D_middle = np.vstack([I_stack1, zero_stack1, zero_stack2, zero_stack2])\n",
    "    D_right = np.vstack([zero_stack1, zero_stack1, -I_stack2, -I_stack2])\n",
    "\n",
    "    D = np.hstack([D_left, D_middle, D_right])\n",
    "    b = np.zeros((n_states*(n_actions-1)*2 + 2*n_states, 1))\n",
    "    bounds = np.array([(None, None)]*2*n_states + [(-Rmax, Rmax)]*n_states)\n",
    "\n",
    "    # We still need to bound R. To do this, we just add\n",
    "    # -I R <= Rmax 1\n",
    "    # I R <= Rmax 1\n",
    "    # So to D we need to add -I and I, and to b we need to add Rmax 1 and Rmax 1\n",
    "    D_bounds = np.hstack([\n",
    "        np.vstack([\n",
    "            -np.eye(n_states),\n",
    "            np.eye(n_states)]),\n",
    "        np.vstack([\n",
    "            np.zeros((n_states, n_states)),\n",
    "            np.zeros((n_states, n_states))]),\n",
    "        np.vstack([\n",
    "            np.zeros((n_states, n_states)),\n",
    "            np.zeros((n_states, n_states))])])\n",
    "    b_bounds = np.vstack([Rmax*np.ones((n_states, 1))]*2)\n",
    "    D = np.vstack((D, D_bounds))\n",
    "    b = np.vstack((b, b_bounds))\n",
    "    A_ub = matrix(D)\n",
    "    b = matrix(b)\n",
    "    c = matrix(c)\n",
    "    results = solvers.lp(c, A_ub, b)\n",
    "    r = np.asarray(results[\"x\"][:n_states], dtype=np.double)\n",
    "\n",
    "    return r.reshape((n_states,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(grid_size, l1):\n",
    "    \"\"\"\n",
    "    Run linear programming inverse reinforcement learning on the gridworld MDP.\n",
    "    Plots the reward function.\n",
    "    grid_size: Grid size. int.\n",
    "    discount: MDP discount factor. float.\n",
    "    \"\"\"\n",
    "\n",
    "    wind = 0.3\n",
    "    trajectory_length = 3*grid_size\n",
    "\n",
    "    gw = Gridworld(grid_size, wind, 0.2)\n",
    "\n",
    "    ground_r = np.array([gw.reward(s) for s in range(gw.n_states)])\n",
    "    policy = [gw.optimal_policy_deterministic(s) for s in range(gw.n_states)]\n",
    "    r = irl(gw.n_states, gw.n_actions, gw.transition_probability,\n",
    "            policy, gw.discount, 1, l1)\n",
    "    \n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pcolor(ground_r.reshape((grid_size, grid_size)))\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Groundtruth reward\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pcolor(r.reshape((grid_size, grid_size)))\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Recovered reward\")\n",
    "    plt.show()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -6.5658e+01  4e+02  2e+00  7e+00  1e+00\n",
      " 1: -9.0720e-01 -1.3791e+01  5e+01  5e-01  2e+00  1e+00\n",
      " 2: -1.8614e+00 -6.0577e+00  1e+01  2e-01  5e-01  3e-01\n",
      " 3: -2.5275e+00 -4.0287e+00  5e+00  5e-02  2e-01  1e-01\n",
      " 4: -2.8839e+00 -3.3451e+00  1e+00  2e-02  5e-02  3e-02\n",
      " 5: -3.0320e+00 -3.2038e+00  5e-01  6e-03  2e-02  1e-02\n",
      " 6: -3.0467e+00 -3.1523e+00  3e-01  4e-03  1e-02  3e-03\n",
      " 7: -3.0921e+00 -3.1187e+00  8e-02  9e-04  3e-03  8e-04\n",
      " 8: -3.1033e+00 -3.1065e+00  1e-02  1e-04  4e-04  4e-05\n",
      " 9: -3.1050e+00 -3.1050e+00  2e-04  2e-06  8e-06  8e-07\n",
      "10: -3.1050e+00 -3.1050e+00  2e-06  2e-08  8e-08  8e-09\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = main(5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.26549300e-09, -1.59804034e-06, -1.31850600e-10,\n",
       "        -1.51879878e-07, -1.48441546e-07],\n",
       "       [ 6.95960136e-04,  4.11082582e-03, -3.52689099e-09,\n",
       "        -7.94960710e-10, -9.18166253e-10],\n",
       "       [-1.02548836e-05,  4.35343747e-03,  2.56610326e-02,\n",
       "        -1.95641595e-09, -6.61843823e-10],\n",
       "       [-1.09650591e-05, -2.65271372e-09,  2.70904208e-02,\n",
       "         1.60190629e-01, -1.10974051e-09],\n",
       "       [-1.25445045e-09,  3.15285900e-09,  1.03220907e-09,\n",
       "         1.42886026e-01,  1.00000000e+00]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.reshape(5,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
