{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Board\n",
    "\n",
    "The Board class will have the following values.\n",
    "1. Number of rows\n",
    "2. Number of columns\n",
    "3. Win State\n",
    "4. Lose State\n",
    "5. Start State\n",
    "6. Obstacle\n",
    "\n",
    "It will have the following methods:\n",
    "1. is_win_state : to verify whether the winning state is reached\n",
    "2. is_lose_state : to verify whether the loosing state is reached\n",
    "3. is_not_obstacle : to verify whther the obstacle is reached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__(self,board_row,board_column,win_state,lose_state,start_state,obstacle):\n",
    "        self.board_row = board_row\n",
    "        self.board_column = board_column\n",
    "        self.win_state = win_state\n",
    "        self.lose_state = lose_state\n",
    "        self.start_state = start_state\n",
    "        self.obstacle = obstacle\n",
    "    \n",
    "    def is_win_state (self,state):\n",
    "        return (self.win_state == state)\n",
    "    def is_lose_state (self,state):\n",
    "        return (self.lose_state == state)\n",
    "    def is_not_obstacle (self,state):\n",
    "        return (self.obstacle != state)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "The environment will have the following values:\n",
    "1. Action : the action picked by the agent\n",
    "2. reward : the reward for the action picked\n",
    "3. state : the current state of the agent\n",
    "4. board : the board configuration\n",
    "5. gamma : the discount factor\n",
    "\n",
    "It will have the following methods \n",
    "1. get_curr_reward: returns the reward of the current state\n",
    "2. get_next_state: returns the next state corresponding to the action taken by the agent in the current state.\n",
    "3. reset: to reset the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Envirnonment:\n",
    "    def __init__(self,action,board,gamma,reward = 0):\n",
    "        self.action = action\n",
    "        self.reward = 0\n",
    "        self.board = board\n",
    "        self.state = board.start_state\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = board.start_state\n",
    "        self.reward = 0\n",
    "        \n",
    "    def get_curr_reward(self,state):\n",
    "        if self.board.is_win_state(state) :\n",
    "            return 1\n",
    "        if self.board.is_lose_state(state) :\n",
    "            return -1\n",
    "        else:\n",
    "            return self.gamma\n",
    "    \n",
    "    def get_next_state(self,action):\n",
    "        val = np.random.rand()\n",
    "        if action == \"up\":\n",
    "            \n",
    "            if val<=0.8:\n",
    "                next_state = (self.state[0]+1,self.state[1])\n",
    "            elif val<=0.9:\n",
    "                next_state = (self.state[0],self.state[1]-1)\n",
    "            elif val<=1:\n",
    "                next_state = (self.state[0],self.state[1]+1)\n",
    "        elif action == \"down\":\n",
    "            if val<=0.8:\n",
    "                next_state = (self.state[0]-1,self.state[1])\n",
    "            elif val<=0.9:\n",
    "                next_state = (self.state[0],self.state[1]-1)\n",
    "            elif val<=1:\n",
    "                next_state = (self.state[0],self.state[1]+1)\n",
    "        elif action == \"left\":\n",
    "            if val<=0.8:\n",
    "                next_state = (self.state[0],self.state[1]-1)\n",
    "            elif val<=0.9:\n",
    "                next_state = (self.state[0]-1,self.state[1])\n",
    "            elif val<=1:\n",
    "                next_state = (self.state[0]+1,self.state[1])\n",
    "        else:\n",
    "            if val<=0.8:\n",
    "                next_state = (self.state[0],self.state[1]+1)\n",
    "            elif val<=0.9:\n",
    "                next_state = (self.state[0]-1,self.state[1])\n",
    "            elif val<=1:\n",
    "                next_state = (self.state[0]+1,self.state[1])\n",
    "        \n",
    "        if (next_state[0] >= 0) and (next_state[0] <= 2):\n",
    "                if (next_state[1] >= 0) and (next_state[1] <= 3):\n",
    "                    if board.is_not_obstacle(next_state):\n",
    "                        return next_state\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "The agent class will have the following entities:\n",
    "\n",
    "1. state: The start state of the agent\n",
    "2. env: The environment in which the agent is\n",
    "3. state_value: dictionary of value functions for a state\n",
    "4. learning_rate: the value of learning rate\n",
    "5. exploration_rate: the value of exploration for using explore exploit.\n",
    "6. action: the current action of the agent \n",
    "7. board: the board for playing the game.\n",
    "8. reward: value of reward for the agent.\n",
    "9. state_store: the array of states visited by the agent.\n",
    "\n",
    "The Agent class will have the following methods:\n",
    "1. play: The method to play the game and learn.\n",
    "2. select_action: The method to pick an action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,learning_rate,exploration_rate,board):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.action = self.select_action()\n",
    "        self.board = board\n",
    "        self.state = board.start_state\n",
    "        self.env = Envirnonment(self.action,self.board,-0.1,0)\n",
    "        self.state_value = {}\n",
    "        self.reward = -1000\n",
    "        self.state_store = []\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = board.start_state\n",
    "        self.reward = -1000\n",
    "        self.env.reset()\n",
    "    \n",
    "    def play(self, rounds = 10):\n",
    "        i = 0\n",
    "        epoch = 10\n",
    "        while i<epoch:\n",
    "            \n",
    "            if self.board.is_win_state(self.state):\n",
    "#                 print(\"hello 1\")\n",
    "                reward = self.env.get_curr_reward(self.state)\n",
    "                \n",
    "                self.state_value[self.state] = reward\n",
    "                print(\"Game Ends , Final Reward\", reward)\n",
    "                print(\"Epoch number \",i)\n",
    "                \n",
    "                for s in reversed(self.state_store):\n",
    "                    reward = self.state_value[s] + self.learning_rate * (reward - self.state_value[s])\n",
    "                    self.state_value[s] = round(reward,3)\n",
    "                self.reset()\n",
    "                i+=1\n",
    "            else:\n",
    "#                 print(\"hello 2\")\n",
    "                action = self.select_action()\n",
    "                self.state_store.append(self.env.get_next_state(action))\n",
    "                print(\"Current position {}  action {}\".format(self.env.state,action))\n",
    "                self.env.state = self.state_store[-1]\n",
    "                self.state = self.state_store[-1]\n",
    "                self.state_value[self.state] = self.env.get_curr_reward(self.state)\n",
    "    def select_action(self):\n",
    "        my_next_reward = -100\n",
    "        action = \"\"\n",
    "        \n",
    "        if np.random.uniform(0,1) <= self.exploration_rate:\n",
    "            action = np.random.choice([\"up\",\"down\",\"left\",\"right\"])\n",
    "        else:\n",
    "            \n",
    "            for a in [\"up\",\"down\",\"left\",\"right\"]:\n",
    "                temp_state = self.env.get_next_state(a)\n",
    "                if temp_state in self.state_value.keys() :\n",
    "                    next_reward = self.state_value[temp_state]\n",
    "                else :\n",
    "                    self.state_value[temp_state] = 0\n",
    "                    next_reward = self.state_value[temp_state]\n",
    "                if next_reward >= my_next_reward:\n",
    "                    action = a\n",
    "                    my_next_reward = next_reward\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Board(3,4,(2,3),(1,3),(0,0),(1,1))\n",
    "agent = Agent(learning_rate = 0.1,exploration_rate = 0.5,board = board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position (0, 0)  action up\n",
      "Current position (1, 0)  action down\n",
      "Current position (0, 0)  action up\n",
      "Current position (1, 0)  action left\n",
      "Current position (2, 0)  action left\n",
      "Current position (2, 0)  action right\n",
      "Current position (2, 1)  action down\n",
      "Current position (2, 1)  action right\n",
      "Current position (2, 1)  action right\n",
      "Current position (2, 1)  action right\n",
      "Current position (2, 2)  action right\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  0\n",
      "Current position (0, 0)  action down\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 0)  action up\n",
      "Current position (1, 0)  action up\n",
      "Current position (2, 0)  action right\n",
      "Current position (2, 1)  action down\n",
      "Current position (2, 1)  action up\n",
      "Current position (2, 1)  action left\n",
      "Current position (2, 0)  action up\n",
      "Current position (2, 0)  action right\n",
      "Current position (1, 0)  action right\n",
      "Current position (1, 0)  action down\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 0)  action down\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action right\n",
      "Current position (0, 3)  action left\n",
      "Current position (0, 2)  action right\n",
      "Current position (0, 3)  action up\n",
      "Current position (1, 3)  action up\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  1\n",
      "Current position (0, 0)  action left\n",
      "Current position (0, 0)  action down\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action up\n",
      "Current position (0, 2)  action up\n",
      "Current position (1, 2)  action up\n",
      "Current position (2, 2)  action down\n",
      "Current position (1, 2)  action up\n",
      "Current position (1, 2)  action right\n",
      "Current position (1, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action left\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action down\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action left\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action right\n",
      "Current position (0, 3)  action left\n",
      "Current position (0, 2)  action right\n",
      "Current position (0, 3)  action left\n",
      "Current position (0, 2)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (1, 3)  action up\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  2\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 0)  action right\n",
      "Current position (1, 0)  action up\n",
      "Current position (2, 0)  action down\n",
      "Current position (1, 0)  action right\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 0)  action down\n",
      "Current position (0, 0)  action left\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action up\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 0)  action left\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action up\n",
      "Current position (0, 3)  action down\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action down\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action down\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action left\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action down\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action left\n",
      "Current position (1, 3)  action up\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  3\n",
      "Current position (0, 0)  action down\n",
      "Current position (0, 0)  action left\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 0)  action up\n",
      "Current position (1, 0)  action up\n",
      "Current position (2, 0)  action down\n",
      "Current position (1, 0)  action right\n",
      "Current position (1, 0)  action right\n",
      "Current position (1, 0)  action right\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action up\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 1)  action up\n",
      "Current position (0, 1)  action up\n",
      "Current position (0, 1)  action left\n",
      "Current position (0, 0)  action left\n",
      "Current position (0, 0)  action down\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action down\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action up\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action left\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action right\n",
      "Current position (0, 3)  action down\n",
      "Current position (0, 3)  action right\n",
      "Current position (1, 3)  action left\n",
      "Current position (1, 2)  action up\n",
      "Current position (2, 2)  action right\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  4\n",
      "Current position (0, 0)  action up\n",
      "Current position (1, 0)  action up\n",
      "Current position (1, 0)  action down\n",
      "Current position (0, 0)  action up\n",
      "Current position (1, 0)  action right\n",
      "Current position (1, 0)  action up\n",
      "Current position (2, 0)  action up\n",
      "Current position (2, 0)  action right\n",
      "Current position (2, 1)  action down\n",
      "Current position (2, 2)  action left\n",
      "Current position (2, 1)  action down\n",
      "Current position (2, 1)  action right\n",
      "Current position (2, 2)  action right\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  5\n",
      "Current position (0, 0)  action left\n",
      "Current position (0, 0)  action left\n",
      "Current position (0, 0)  action down\n",
      "Current position (0, 0)  action up\n",
      "Current position (0, 0)  action up\n",
      "Current position (1, 0)  action down\n",
      "Current position (0, 0)  action down\n",
      "Current position (0, 0)  action up\n",
      "Current position (1, 0)  action up\n",
      "Current position (1, 0)  action left\n",
      "Current position (1, 0)  action left\n",
      "Current position (1, 0)  action right\n",
      "Current position (2, 0)  action right\n",
      "Current position (2, 1)  action right\n",
      "Current position (2, 2)  action up\n",
      "Current position (2, 2)  action right\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  6\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action up\n",
      "Current position (1, 2)  action up\n",
      "Current position (2, 2)  action right\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  7\n",
      "Current position (0, 0)  action left\n",
      "Current position (1, 0)  action left\n",
      "Current position (2, 0)  action right\n",
      "Current position (2, 1)  action right\n",
      "Current position (2, 2)  action down\n",
      "Current position (1, 2)  action left\n",
      "Current position (1, 2)  action right\n",
      "Current position (1, 3)  action up\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  8\n",
      "Current position (0, 0)  action up\n",
      "Current position (0, 0)  action right\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action down\n",
      "Current position (0, 1)  action up\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 1)  action right\n",
      "Current position (0, 2)  action left\n",
      "Current position (1, 2)  action right\n",
      "Current position (1, 3)  action right\n",
      "Current position (1, 3)  action left\n",
      "Game Ends , Final Reward 1\n",
      "Epoch number  9\n"
     ]
    }
   ],
   "source": [
    "agent.play(rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(agent.select_action())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.106 -0.127 -0.168 -0.22  \n",
      "-0.084 0.0 -0.115 -0.293  \n",
      "-0.042 0.068 0.248 0.325  \n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if i==1 and j==1:\n",
    "            print(0.00,end = \" \")\n",
    "        else:\n",
    "            print(agent.state_value[(i,j)],end = \" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
