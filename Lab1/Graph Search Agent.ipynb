{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Search Agent\n",
    "\n",
    "The graph search agent requires an environment to define the following\n",
    "\n",
    "1. Start State\n",
    "2. Goal State\n",
    "3. Possible Actions\n",
    "\n",
    "We have to make a generalised agent, which reaches the goal state using the functions of the environment. Our agent will use BFS/DFS to reach to the goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node\n",
    "The Node class greates the graph node. It has the following values\n",
    "1. Parent Node\n",
    "2. State \n",
    "3. pcost - Path Cost\n",
    "4. hcost - Heuristic Cost\n",
    "5. cost - Total cost = pcost + hcost\n",
    "\n",
    "It makes use of the following built in functions: \n",
    "1. \\_\\_hash\\_\\_ : This provides the hash value for every node, which is required for the hashset\n",
    "2. \\_\\_eq\\_\\_ : To check if 2 nodes are equal (Operator overload)\n",
    "3. \\_\\_ne\\_\\_ : To check if 2 nodes are not equal (Operator overload)\n",
    "4. \\_\\_str\\_\\_ : To get string representation of state in node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, parent, state, pcost, hcost):\n",
    "        \n",
    "        self.parent = parent\n",
    "        self.state = state\n",
    "        self.pcost = pcost\n",
    "        self.hcost = hcost\n",
    "        self.cost = pcost + hcost\n",
    "    \n",
    "    def __hash__(self):\n",
    "        \n",
    "        return hash(''.join(self.state.flatten()))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.state)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \n",
    "        return hash(''.join(self.state.flatten())) == hash(''.join(other.state.flatten())) \n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return hash(''.join(self.state.flatten())) != hash(''.join(other.state.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PriorityQueue\n",
    "The Priority Queue is used to store the nodes along with the cost, and pop the node having the least cost for BFS\n",
    "\n",
    "It makes use of the following functions: \n",
    "1. push : Add node to queue\n",
    "2. pop : Pop node having least cost\n",
    "3. is_empty : To check if queue is empty\n",
    "4. \\_\\_len\\_\\_ : To get length of queue\n",
    "5. \\_\\_str\\_\\_ : To get string representation of queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityQueue():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.queue = []\n",
    "        \n",
    "    def push(self, node):\n",
    "        self.queue.append(node)\n",
    "    \n",
    "    def pop(self):\n",
    "        \n",
    "        next_state = None\n",
    "        state_cost = 10**18\n",
    "        index = -1\n",
    "        \n",
    "        for i in range(len(self.queue)):\n",
    "            \n",
    "            if self.queue[i].cost<state_cost:\n",
    "                state_cost = self.queue[i].cost\n",
    "                index = i\n",
    "        \n",
    "        return self.queue.pop(index)\n",
    "    \n",
    "    def is_empty(self):\n",
    "        \n",
    "        return len(self.queue)==0\n",
    "    \n",
    "    def __str__(self):\n",
    "        l = []\n",
    "        for i in self.queue:\n",
    "            l.append(i.state)\n",
    "        \n",
    "        return str(l)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.queue)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "The environment is what the agent plays in. It has the following entities:\n",
    "1. actions : The actions defined in the environment\n",
    "2. depth: the maximum depth of the solution\n",
    "3. goal_state : The goal state of the environment\n",
    "4. start_state : The start state generated at the depth\n",
    "\n",
    "It has the following functions: \n",
    "1. get_start_state : returns the start state\n",
    "2. reached_goal : returns goal_state\n",
    "3. get_next_states : Given current state, it returns all possible next states\n",
    "4. generate_start_state : Given goal state and depth d, performs d moves to generate a start state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self, depth = None, goal_state = None):\n",
    "        self.actions = [1,2,3,4] #1 - Up, 2 - Down, 3 - Right, 4 - Left\n",
    "        self.goal_state = goal_state\n",
    "        self.depth = depth\n",
    "        self.start_state = self.generate_start_state()\n",
    "    \n",
    "    def generate_start_state(self):\n",
    "        \n",
    "        past_state = goal_state\n",
    "        i=0\n",
    "        while i!= self.depth:\n",
    "            new_states = self.get_next_states(past_state)\n",
    "            choice = np.random.randint(low=0, high=len(new_states))\n",
    "            \n",
    "            if np.array_equal(new_states[choice], past_state):\n",
    "                continue\n",
    "            \n",
    "            past_state = new_states[choice]\n",
    "            i+=1\n",
    "            \n",
    "        return past_state\n",
    "    \n",
    "    def get_start_state(self):\n",
    "        return self.start_state\n",
    "    \n",
    "    def get_goal_state(self):\n",
    "        return self.goal_state\n",
    "    \n",
    "    def get_next_states(self, state):\n",
    "        \n",
    "        space = (0,0)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state[i,j] == '_':\n",
    "                    space = (i,j)\n",
    "                    break\n",
    "        \n",
    "        new_states = []\n",
    "        \n",
    "        if space[0] > 0:# Move Up\n",
    "            new_state = np.copy(state)\n",
    "            \n",
    "            val = new_state[space[0], space[1]]\n",
    "            new_state[space[0], space[1]]  = new_state[space[0]-1, space[1]]\n",
    "            new_state[space[0]-1, space[1]] = val\n",
    "            \n",
    "            new_states.append(new_state)\n",
    "            \n",
    "        if space[0] < 2: #Move down\n",
    "            new_state = np.copy(state)\n",
    "            \n",
    "            val = new_state[space[0], space[1]]\n",
    "            new_state[space[0], space[1]]  = new_state[space[0]+1, space[1]]\n",
    "            new_state[space[0]+1, space[1]] = val\n",
    "            \n",
    "            new_states.append(new_state)\n",
    "        \n",
    "        if space[1]<2: #Move right\n",
    "            new_state = np.copy(state)\n",
    "            \n",
    "            val = new_state[space[0], space[1]]\n",
    "            new_state[space[0], space[1]] = new_state[space[0], space[1]+1]\n",
    "            new_state[space[0], space[1]+1] = val\n",
    "            \n",
    "            new_states.append(new_state)\n",
    "            \n",
    "        if space[1] > 0: #Move Left\n",
    "            new_state = np.copy(state)\n",
    "            \n",
    "            val = new_state[space[0], space[1]]\n",
    "            new_state[space[0], space[1]] = new_state[space[0], space[1]-1]\n",
    "            new_state[space[0], space[1]-1] = val\n",
    "            \n",
    "            new_states.append(new_state)\n",
    "        \n",
    "        return new_states\n",
    "    \n",
    "    def reached_goal(self, state):\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state[i,j] != self.goal_state[i,j]:\n",
    "                    return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "The agent is the player who plays the game against the environment to win. It has the following entities:\n",
    "1. frontier : This is the priority queue used to store the nodes to be explored.\n",
    "2. explored : This is the dictionary which stores the explored nodes\n",
    "3. start_state : Stores the start state\n",
    "4. goal_state : Stores the goal state\n",
    "5. env : Stores the environment\n",
    "6. goal_node : Stores the goal node if found\n",
    "7. heuristic : Stores the heuristic function\n",
    "\n",
    "The agent has the following functions: \n",
    "1. run(): Is the function that explores the environment and finds the goal node. Uses the built in heuristic function to get the path costs\n",
    "2. print_nodes(): To print the path from the start node to goal node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, heuristic):\n",
    "        self.frontier = PriorityQueue()\n",
    "        self.explored = dict()\n",
    "        self.start_state = env.get_start_state()\n",
    "        self.goal_state = env.get_goal_state()\n",
    "        self.env = env\n",
    "        self.goal_node = None\n",
    "        self.heuristic = heuristic\n",
    "    \n",
    "    def run(self):\n",
    "        init_node = Node(parent = None, state = self.start_state, pcost = 0, hcost=0)\n",
    "        self.frontier.push(init_node)\n",
    "        steps = 0\n",
    "        while not self.frontier.is_empty():\n",
    "\n",
    "            curr_node = self.frontier.pop()\n",
    "            #print(curr_node.cost)\n",
    "            next_states = self.env.get_next_states(curr_node.state)\n",
    "\n",
    "            if hash(curr_node) in self.explored:\n",
    "                continue\n",
    "\n",
    "            self.explored[hash(curr_node)] = curr_node\n",
    "\n",
    "            if self.env.reached_goal(curr_node.state):\n",
    "                #print(\"Reached goal!\")\n",
    "                self.goal_node = curr_node\n",
    "                break\n",
    "            goal_state = self.env.get_goal_state()\n",
    "\n",
    "            l = []\n",
    "            for state in next_states:\n",
    "\n",
    "                hcost = self.heuristic(state, goal_state)\n",
    "                node = Node(parent=curr_node, state=state, pcost=curr_node.pcost+1, hcost=hcost)\n",
    "                self.frontier.push(node)\n",
    "            steps += 1\n",
    "        \n",
    "        \n",
    "        return steps, self.soln_depth()\n",
    "    \n",
    "    def soln_depth(self):\n",
    "        node = self.goal_node\n",
    "        count = 0\n",
    "        while node is not None:\n",
    "            node = node.parent\n",
    "            count+=1\n",
    "        \n",
    "        return count\n",
    "    \n",
    "    def print_nodes(self):\n",
    "        \n",
    "        node = self.goal_node\n",
    "        l = []\n",
    "        while node is not None:\n",
    "            l.append(node)\n",
    "            node = node.parent\n",
    "\n",
    "        step = 1\n",
    "        for node in l[::-1]:\n",
    "            print(\"Step: \",step)\n",
    "            print(node)\n",
    "            step+=1\n",
    "    \n",
    "    def get_memory(self):\n",
    "        \n",
    "        mem = len(self.frontier)*56 + len(self.explored)*56\n",
    "        return mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the environment cannot spawn its own start and end states, we input the start state and end state to the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic 0\n",
    "This is a null heuristic, returns 0 for any state. Essentially uniform cost search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic0(curr_state, goal_state):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic 1\n",
    "\n",
    "This heuristic counts the number of misplaced tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic1(curr_state, goal_state):\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if curr_state[i, j]!=goal_state[i,j]:\n",
    "                count+=1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic 2\n",
    "\n",
    "This is the manhattan distance, it returns the sum of the horizontal and vertical distances of the all marble in current state from center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic2(curr_state, goal_state):\n",
    "    \n",
    "    dist = 0\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ele = curr_state[i, j]\n",
    "            goal_i, goal_j = np.where(goal_state==ele)\n",
    "            d = abs(goal_i[0] - i) + abs(goal_j[0] - j)\n",
    "            dist += d\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start State: \n",
      "[['2' '1' '3']\n",
      " ['8' '_' '6']\n",
      " ['7' '4' '5']]\n",
      "Goal State: \n",
      "[['1' '2' '3']\n",
      " ['8' '_' '4']\n",
      " ['7' '6' '5']]\n"
     ]
    }
   ],
   "source": [
    "depth = 500\n",
    "goal_state = np.array([[1,2,3], [8,'_',4], [7,6,5]])\n",
    "env = Environment(depth, goal_state)\n",
    "print(\"Start State: \")\n",
    "print(env.get_start_state())\n",
    "print(\"Goal State: \")\n",
    "print(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env = env, heuristic = heuristic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1420, 21)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.979869842529297e-05 56.0\n",
      "50 0.013271903991699219 9558.08\n",
      "100 0.10117329120635986 46243.68\n",
      "150 0.5259712839126587 128345.28\n",
      "200 1.6053526878356934 147803.04\n",
      "250 0.5069449663162231 138197.92\n",
      "300 0.6685653352737426 137922.4\n",
      "350 0.6027336311340332 140533.12\n",
      "400 0.5257553243637085 123772.32\n",
      "450 0.5634418106079102 139392.96\n",
      "500 0.5087827444076538 122263.68\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(0,501,50)\n",
    "goal_state = np.array([[1,2,3], [8,'_',4], [7,6,5]])\n",
    "times_taken = {}\n",
    "mems = {}\n",
    "for depth in depths:\n",
    "    \n",
    "    time_taken = 0\n",
    "    mem = 0\n",
    "    for i in range(50):\n",
    "        env = Environment(depth=depth, goal_state=goal_state)\n",
    "        agent = Agent(env = env, heuristic = heuristic2)\n",
    "        start_time = time()\n",
    "        agent.run()\n",
    "        end_time = time()\n",
    "        time_taken+=end_time - start_time\n",
    "        mem+=agent.get_memory()\n",
    "    \n",
    "    time_taken/=50\n",
    "    mem = mem/50\n",
    "    times_taken[depth] = time_taken\n",
    "    mems[depth] = mem\n",
    "    print(depth, time_taken, mem)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
